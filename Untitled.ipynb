{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "from distributed.utils import parse_bytes, format_bytes\n",
    "import math\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set({'array.chunk-size': '50 MiB'}):\n",
    "        x = da.ones((10000, 10000))\n",
    "        assert 4 < x.npartitions < 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries(\n",
    "    chunk_size='128 MB',\n",
    "    num_nodes=1,\n",
    "    worker_per_node=1,\n",
    "    chunking_scheme=None,\n",
    "    lat=320,\n",
    "    lon=384,\n",
    "    start='1980-01-01',\n",
    "    freq='1D',\n",
    "    nan=False,\n",
    "):\n",
    "    \"\"\" Create synthetic Xarray dataset filled with random\n",
    "    data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_size : str\n",
    "          chunk size in bytes, kilo, mega or any factor of bytes\n",
    "    num_nodes : int\n",
    "           number of compute nodes\n",
    "    worker_per_node: int\n",
    "           number of dask workers per node\n",
    "\n",
    "    chunking_scheme : str\n",
    "           Whether to chunk across time dimension ('temporal') or horizontal dimensions (lat, lon) ('spatial').\n",
    "           If None, automatically determine chunk sizes along all dimensions. \n",
    "           \n",
    "    lat : int\n",
    "         number of latitude values\n",
    "\n",
    "    lon : int\n",
    "         number of longitude values\n",
    "\n",
    "    start : datetime (or datetime-like string)\n",
    "        Start of time series\n",
    "\n",
    "    freq : string\n",
    "        String like '2s' or '1H' or '12W' for the time series frequency\n",
    "    nan : bool\n",
    "         Whether to include nan in generated data\n",
    "\n",
    "\n",
    "    Examples\n",
    "    ---------\n",
    "\n",
    "    >>> from benchmarks.datasets import timeseries\n",
    "    >>> ds = timeseries('128MB', 5, chunking_scheme='spatial', lat=500, lon=600)\n",
    "    >>> ds\n",
    "    <xarray.Dataset>\n",
    "    Dimensions:  (lat: 500, lon: 600, time: 267)\n",
    "    Coordinates:\n",
    "    * time     (time) datetime64[ns] 1980-01-01 1980-01-02 ... 1980-09-23\n",
    "    * lon      (lon) float64 -180.0 -179.4 -178.8 -178.2 ... 178.8 179.4 180.0\n",
    "    * lat      (lat) float64 -90.0 -89.64 -89.28 -88.92 ... 88.92 89.28 89.64 90.0\n",
    "    Data variables:\n",
    "        sst      (time, lon, lat) float64 dask.array<shape=(267, 600, 500), chunksize=(267, 245, 245)>\n",
    "    Attributes:\n",
    "        history:  created for compute benchmarking\n",
    "    \"\"\"\n",
    "\n",
    "    dt = np.dtype('f8')\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = parse_bytes(chunk_size)\n",
    "    total_bytes = chunk_size * num_nodes * worker_per_node\n",
    "    size = total_bytes / itemsize\n",
    "    timesteps = math.ceil(size / (lat * lon))\n",
    "    shape = (timesteps, lon, lat)\n",
    "    if chunking_scheme=='temporal':\n",
    "        x = math.ceil(chunk_size / (lon * lat * itemsize))\n",
    "        chunks = (x, lon, lat)\n",
    "    elif chunking_scheme=='spatial':\n",
    "        x = math.ceil(math.sqrt(chunk_size / (timesteps * itemsize)))\n",
    "        chunks = (timesteps, x, x)\n",
    "    else:\n",
    "        chunks = 'auto'\n",
    "        \n",
    "    lats = xr.DataArray(np.linspace(start=-90, stop=90, num=lat), dims=['lat'])\n",
    "    lons = xr.DataArray(np.linspace(start=-180, stop=180, num=lon), dims=['lon'])\n",
    "    times = xr.DataArray(pd.date_range(start=start, freq=freq, periods=timesteps), dims=['time'])\n",
    "    if chunks== 'auto':\n",
    "        with dask.config.set({'array.chunk-size': chunk_size}):\n",
    "            random_data = randn(shape=shape, chunks=chunks, nan=nan)\n",
    "    else:\n",
    "        random_data = randn(shape=shape, chunks=chunks, nan=nan)\n",
    "    ds = xr.DataArray(\n",
    "        random_data,\n",
    "        dims=['time', 'lon', 'lat'],\n",
    "        coords={'time': times, 'lon': lons, 'lat': lats},\n",
    "        name='sst',\n",
    "        encoding=None,\n",
    "        attrs={'units': 'baz units', 'description': 'a description'},\n",
    "    ).to_dataset()\n",
    "    ds.attrs = {'history': 'created for compute benchmarking'}\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def randn(shape, chunks=None, nan=False, seed=0):\n",
    "    rng = da.random.RandomState(seed)\n",
    "    x = 5 + 3 * rng.standard_normal(shape, chunks=chunks)\n",
    "    if nan:\n",
    "        x = da.where(x < 0, np.nan, x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 500, lon: 600, time: 214)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1980-01-01 1980-01-02 ... 1980-08-01\n",
       "  * lon      (lon) float64 -180.0 -179.4 -178.8 -178.2 ... 178.8 179.4 180.0\n",
       "  * lat      (lat) float64 -90.0 -89.64 -89.28 -88.92 ... 88.92 89.28 89.64 90.0\n",
       "Data variables:\n",
       "    sst      (time, lon, lat) float64 dask.array<shape=(214, 600, 500), chunksize=(107, 150, 125)>\n",
       "Attributes:\n",
       "    history:  created for compute benchmarking"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = timeseries('64MB', 8, chunking_scheme=None, lat=500, lon=600)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 500, lon: 600, time: 267)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1980-01-01 1980-01-02 ... 1980-09-23\n",
       "  * lon      (lon) float64 -180.0 -179.4 -178.8 -178.2 ... 178.8 179.4 180.0\n",
       "  * lat      (lat) float64 -90.0 -89.64 -89.28 -88.92 ... 88.92 89.28 89.64 90.0\n",
       "Data variables:\n",
       "    sst      (time, lon, lat) float64 dask.array<shape=(267, 600, 500), chunksize=(251, 200, 250)>\n",
       "Attributes:\n",
       "    history:  created for compute benchmarking"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries('128MB', 5, lat=500, lon=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128420352"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "261*248*248*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<add, shape=(10000, 10000, 1000), dtype=float64, chunksize=(125, 125, 125)>\n"
     ]
    }
   ],
   "source": [
    "with dask.config.set({'array.chunk-size': '50 MiB'}):\n",
    "    x = randn((10000, 10000, 1000), chunks='auto')\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'125.00 MB'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_bytes(250*250*250*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.dtype('f8')\n",
    "itemsize = dt.itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis]",
   "language": "python",
   "name": "conda-env-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
